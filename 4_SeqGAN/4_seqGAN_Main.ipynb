{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "012_seqGAN .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZFt49LLrZpQ"
      },
      "source": [
        "## SeqGAN 구현 코드\n",
        "- 전처리된 데이터 필요\n",
        "- 이전 Data Preprocessing 코드를 통해 자동으로 생성되어 코랩에 저장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lefXJZWTqy-q",
        "outputId": "aa4555e5-ec66-4abc-e3b6-26b71e602fca"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr 23 03:02:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjmnq6hmrlkD"
      },
      "source": [
        "## 1. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7emNDaAmgI"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt4L0Tq0ArkN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjsKLvmfBFrU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPJioB8gqfk"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNtegCDeryR9"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zhpLA77rwhQ"
      },
      "source": [
        "## 2. Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4yJr7JRrrsR",
        "outputId": "01b6f89d-d0eb-429d-d4e1-584ccf223b3d"
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd-gbMEwr6lx",
        "outputId": "311ea236-0a05-400a-cad6-d0d07262e0cd"
      },
      "source": [
        "% cd /content/drive/MyDrive/Colab\\ Notebooks/Project/Lyrics/4_SeqGAN/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Project/Lyrics/4_SeqGAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHYtPMe_cGY"
      },
      "source": [
        "## 3. Generator 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6deYvocc_iQD"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\" Generator \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, use_cuda):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_cuda = use_cuda\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM on the input sequence.\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len), sequence of tokens generated by generator\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len, vocab_size), lstm output prediction\n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        h0, c0 = self.init_hidden(x.size(0))\n",
        "        emb = self.embed(x) # batch_size * seq_len * emb_dim \n",
        "        out, _ = self.lstm(emb, (h0, c0)) # out: batch_size * seq_len * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # (batch_size*seq_len) * vocab_size\n",
        "        return out\n",
        "\n",
        "    def step(self, x, h, c):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM one token at a time (seq_len = 1).\n",
        "        Inputs: x, h, c\n",
        "            - x: (batch_size, 1), sequence of tokens generated by generator\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state\n",
        "        Outputs: out, h, c\n",
        "            - out: (batch_size, vocab_size), lstm output prediction\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state \n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        emb = self.embed(x) # batch_size * 1 * emb_dim\n",
        "        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "        return h, c\n",
        "    \n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.uniform_(-0.05, 0.05)\n",
        "\n",
        "    def sample(self, batch_size, seq_len, x=None):\n",
        "        \"\"\"\n",
        "        Samples the network and returns a batch of samples of length seq_len.\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len)\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        if x is None:\n",
        "            h, c = self.init_hidden(batch_size)\n",
        "            x = torch.zeros(batch_size, 1, dtype=torch.int64)\n",
        "            if self.use_cuda:\n",
        "                x = x.cuda()\n",
        "            for _ in range(seq_len):\n",
        "                out, h, c = self.step(x, h, c)\n",
        "                prob = torch.exp(out)\n",
        "                x = torch.multinomial(prob, 1)\n",
        "                samples.append(x)\n",
        "        else:\n",
        "            h, c = self.init_hidden(x.size(0))\n",
        "            given_len = x.size(1)\n",
        "            lis = x.chunk(x.size(1), dim=1)\n",
        "            for i in range(given_len):\n",
        "                out, h, c = self.step(lis[i], h, c)\n",
        "                samples.append(lis[i])\n",
        "            prob = torch.exp(out)\n",
        "            x = torch.multinomial(prob, 1)\n",
        "            for _ in range(given_len, seq_len):\n",
        "                samples.append(x)\n",
        "                out, h, c = self.step(x, h, c)\n",
        "                prob = torch.exp(out)\n",
        "                x = torch.multinomial(prob, 1)\n",
        "        out = torch.cat(samples, dim=1) # along the batch_size dimension\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdRt324E7TA"
      },
      "source": [
        "## 4. Discriminator 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKvcwbEZE-CO"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    Highway architecture based on the pooled feature maps is added. Dropout is adopted.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, vocab_size, embedding_dim, filter_sizes, num_filters, dropout_prob):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_f, (f_size, embedding_dim)) for f_size, num_f in zip(filter_sizes, num_filters)\n",
        "        ])\n",
        "        self.highway = nn.Linear(sum(num_filters), sum(num_filters))\n",
        "        self.dropout = nn.Dropout(p = dropout_prob)\n",
        "        self.fc = nn.Linear(sum(num_filters), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len)\n",
        "        Outputs: out\n",
        "            - out: (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x).unsqueeze(1) # batch_size, 1 * seq_len * emb_dim\n",
        "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs] # [batch_size * num_filter * seq_len]\n",
        "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]\n",
        "        out = torch.cat(pools, 1)  # batch_size * sum(num_filters)\n",
        "        highway = self.highway(out)\n",
        "        transform = F.sigmoid(highway)\n",
        "        out = transform * F.relu(highway) + (1. - transform) * out # sets C = 1 - T\n",
        "        out = F.log_softmax(self.fc(self.dropout(out)), dim=1) # batch * num_classes\n",
        "        return out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31k0Dp6ZRap-"
      },
      "source": [
        "## 5. Target LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90mCsVA3RccD"
      },
      "source": [
        "class TargetLSTM(nn.Module):\n",
        "    \"\"\" Target LSTM \"\"\"\n",
        "\n",
        "    def __init__(self,  vocab_size, embedding_dim, hidden_dim, use_cuda):\n",
        "        super(TargetLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_cuda = use_cuda\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM on the input sequence.\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len), sequence of tokens generated by generator\n",
        "        Outputs: out\n",
        "            - out: (batch_size, vocab_size), lstm output prediction\n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        h0, c0 = self.init_hidden(x.size(0))\n",
        "        emb = self.embed(x) # batch_size * seq_len * emb_dim \n",
        "        out, _ = self.lstm(emb, (h0, c0)) # out: seq_len * batch_size * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # seq_len * batch_size * vocab_size\n",
        "        return out\n",
        "\n",
        "    def step(self, x, h, c):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM one token at a time (seq_len = 1).\n",
        "        Inputs: x, h, c\n",
        "            - x: (batch_size, 1), sequence of tokens generated by generator\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state\n",
        "        Outputs: out, h, c\n",
        "            - out: (batch_size, 1, vocab_size), lstm output prediction\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state \n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        emb = self.embed(x) # batch_size * 1 * emb_dim\n",
        "        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = torch.zeros((1, batch_size, self.hidden_dim))\n",
        "        c = torch.zeros((1, batch_size, self.hidden_dim))\n",
        "        if self.use_cuda:\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "        return h, c\n",
        "    \n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.normal_(0, 1)\n",
        "\n",
        "    def sample(self, batch_size, seq_len):\n",
        "        \"\"\"\n",
        "        Samples the network and returns a batch of samples of length seq_len.\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len)\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        h, c = self.init_hidden(batch_size)\n",
        "        x = torch.zeros(batch_size, 1, dtype=torch.int64)\n",
        "        if self.use_cuda:\n",
        "            x = x.cuda()\n",
        "        for _ in range(seq_len):\n",
        "            out, h, c = self.step(x, h, c)\n",
        "            prob = torch.exp(out)\n",
        "            x = torch.multinomial(prob, 1)\n",
        "            samples.append(x)\n",
        "        out = torch.cat(samples, dim=1) # along the batch_size dimension\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB2j6Q8BRuje"
      },
      "source": [
        "## 6. PGLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF5SjZL3RwBu"
      },
      "source": [
        "class PGLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Pseudo-loss that gives corresponding policy gradients (on calling .backward()) \n",
        "    for adversial training of Generator\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PGLoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target, reward):\n",
        "        \"\"\"\n",
        "        Inputs: pred, target, reward\n",
        "            - pred: (batch_size, seq_len), \n",
        "            - target : (batch_size, seq_len), \n",
        "            - reward : (batch_size, ), reward of each whole sentence\n",
        "        \"\"\"\n",
        "        one_hot = torch.zeros(pred.size(), dtype=torch.uint8)\n",
        "        if pred.is_cuda:\n",
        "            one_hot = one_hot.cuda()\n",
        "        one_hot.scatter_(1, target.data.view(-1, 1), 1)\n",
        "        loss = torch.masked_select(pred, one_hot)\n",
        "        loss = loss * reward.contiguous().view(-1)\n",
        "        loss = -torch.sum(loss)\n",
        "        return loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fus3bO-SP1V"
      },
      "source": [
        "##7. Data Iter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIQCUQyfSRG6"
      },
      "source": [
        "class GenDataIter:\n",
        "    \"\"\" Toy data iter to load digits \"\"\"\n",
        "\n",
        "    def __init__(self, data_file, batch_size):\n",
        "        super(GenDataIter, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_lis = self.read_file(data_file)\n",
        "        self.data_num = len(self.data_lis)\n",
        "        self.indices = range(self.data_num)\n",
        "        self.num_batches = math.ceil(self.data_num / self.batch_size)\n",
        "        self.idx = 0\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        random.shuffle(self.data_lis)\n",
        "\n",
        "    def next(self):\n",
        "        if self.idx >= self.data_num:\n",
        "            raise StopIteration\n",
        "        index = self.indices[self.idx : self.idx + self.batch_size]\n",
        "        d = [self.data_lis[i] for i in index]\n",
        "        d = torch.tensor(d)\n",
        "\n",
        "        # 0 is prepended to d as start symbol\n",
        "        data = torch.cat([torch.zeros(len(index), 1, dtype=torch.int64), d], dim=1)\n",
        "        target = torch.cat([d, torch.zeros(len(index), 1, dtype=torch.int64)], dim=1)\n",
        "        \n",
        "        self.idx += self.batch_size\n",
        "        return data, target\n",
        "\n",
        "    def read_file(self, data_file):\n",
        "        with open(data_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        lis = []\n",
        "        for line in lines:\n",
        "            l = [int(s) for s in list(line.strip().split())]\n",
        "            lis.append(l)\n",
        "        return lis\n",
        "\n",
        "\n",
        "class DisDataIter:\n",
        "    \"\"\" Toy data iter to load digits \"\"\"\n",
        "\n",
        "    def __init__(self, real_data_file, fake_data_file, batch_size):\n",
        "        super(DisDataIter, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        real_data_lis = self.read_file(real_data_file)\n",
        "        fake_data_lis = self.read_file(fake_data_file)\n",
        "        self.data = real_data_lis + fake_data_lis\n",
        "        self.labels = [1 for _ in range(len(real_data_lis))] +\\\n",
        "                        [0 for _ in range(len(fake_data_lis))]\n",
        "        self.pairs = list(zip(self.data, self.labels))\n",
        "        self.data_num = len(self.pairs)\n",
        "        self.indices = range(self.data_num)\n",
        "        self.num_batches = math.ceil(self.data_num / self.batch_size)\n",
        "        self.idx = 0\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        random.shuffle(self.pairs)\n",
        "\n",
        "    def next(self):\n",
        "        if self.idx >= self.data_num:\n",
        "            raise StopIteration\n",
        "        index = self.indices[self.idx : self.idx + self.batch_size]\n",
        "        pairs = [self.pairs[i] for i in index]\n",
        "        data = [p[0] for p in pairs]\n",
        "        label = [p[1] for p in pairs]\n",
        "        data = torch.tensor(data)\n",
        "        label = torch.tensor(label)\n",
        "        self.idx += self.batch_size\n",
        "        return data, label\n",
        "\n",
        "    def read_file(self, data_file):\n",
        "        with open(data_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        lis = []\n",
        "        for line in lines:\n",
        "            l = [int(s) for s in list(line.strip().split())]\n",
        "            lis.append(l) \n",
        "        return lis"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgbqe0LKVcLC"
      },
      "source": [
        "## 8. Rollout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPazU1vmVdHU"
      },
      "source": [
        "class Rollout(object):\n",
        "    \"\"\" Rollout Policy \"\"\"\n",
        "\n",
        "    def __init__(self, model, update_rate):\n",
        "        self.ori_model = model\n",
        "        self.own_model = copy.deepcopy(model)\n",
        "        self.update_rate = update_rate\n",
        "\n",
        "    def get_reward(self, x, num, discriminator):\n",
        "        \"\"\"\n",
        "        Inputs: x, num, discriminator\n",
        "            - x: (batch_size, seq_len) input data\n",
        "            - num: rollout number\n",
        "            - discriminator: discrimanator model\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        for i in range(num):\n",
        "            for l in range(1, seq_len):\n",
        "                data = x[:, 0:l]\n",
        "                samples = self.own_model.sample(batch_size, seq_len, data)\n",
        "                pred = discriminator(samples)\n",
        "                pred = pred.cpu().data[:,1].numpy()\n",
        "                if i == 0:\n",
        "                    rewards.append(pred)\n",
        "                else:\n",
        "                    rewards[l-1] += pred\n",
        "\n",
        "            # for the last token\n",
        "            pred = discriminator(x)\n",
        "            pred = pred.cpu().data[:, 1].numpy()\n",
        "            if i == 0:\n",
        "                rewards.append(pred)\n",
        "            else:\n",
        "                rewards[seq_len-1] += pred\n",
        "        rewards = np.transpose(np.array(rewards)) / (1.0 * num) # batch_size * seq_len\n",
        "        return rewards\n",
        "\n",
        "    def update_params(self):\n",
        "        dic = {}\n",
        "        for name, param in self.ori_model.named_parameters():\n",
        "            dic[name] = param.data\n",
        "        for name, param in self.own_model.named_parameters():\n",
        "            if name.startswith('emb'):\n",
        "                param.data = dic[name]\n",
        "            else:\n",
        "                param.data = self.update_rate * param.data + (1 - self.update_rate) * dic[name]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DXaHb2CchoW"
      },
      "source": [
        "## 9. SeqGAN 함수 정의\n",
        "- Pre-train Generator\n",
        "- Pre-train Discriminator\n",
        "- Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2-jkRIx2_bu"
      },
      "source": [
        "#########################################################################################\n",
        "#  Generator  Hyper-parameters\n",
        "######################################################################################\n",
        "g_embed_dim = 200 # embedding dimension (pretrained: 200, pk: 30)\n",
        "g_hidden_dim = 300 # hidden state dimension of lstm cell\n",
        "g_seq_len = 20 # sequence length\n",
        "START_TOKEN = 0\n",
        "\n",
        "#########################################################################################\n",
        "#  Discriminator  Hyper-parameters\n",
        "#########################################################################################\n",
        "d_num_class = 2\n",
        "d_embed_dim = 64\n",
        "d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
        "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
        "d_dropout_prob = 0.75\n",
        "\n",
        "DATA_PATH = \"../Data/4_SeqGAN_Data/\"\n",
        "\n",
        "POSITIVE_FILE = DATA_PATH + '4_SeqGAN_positive.txt'\n",
        "NEGATIVE_FILE = DATA_PATH + '4_SeqGAN_negative.txt'\n",
        "\n",
        "def generate_samples(model, batch_size, generated_num, output_file):\n",
        "    samples = []\n",
        "    for _ in range(int(generated_num / batch_size)):\n",
        "        sample = model.sample(batch_size, g_seq_len).cpu().data.numpy().tolist()\n",
        "        samples.extend(sample)\n",
        "    with open(output_file, 'w') as fout:\n",
        "        for sample in samples:\n",
        "            string = ' '.join([str(s) for s in sample])\n",
        "     \n",
        "            fout.write('{}\\n'.format(string))\n",
        "\n",
        "\n",
        "def train_generator_MLE(gen, data_iter, criterion, optimizer, epochs, \n",
        "        gen_pretrain_train_loss, args):\n",
        "    \"\"\"\n",
        "    Train generator with MLE\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = gen(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        data_iter.reset()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    print(\"Epoch {}, train loss: {:.5f}\".format(epoch, avg_loss))\n",
        "    gen_pretrain_train_loss.append(avg_loss)\n",
        "\n",
        "def train_generator_PG(gen, dis, rollout, pg_loss, optimizer, epochs, args):\n",
        "    \"\"\"\n",
        "    Train generator with the guidance of policy gradient\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # construct the input to the genrator, add zeros before samples and delete the last column\n",
        "        samples = generator.sample(args.batch_size, g_seq_len)\n",
        "        zeros = torch.zeros(args.batch_size, 1, dtype=torch.int64)\n",
        "        if samples.is_cuda:\n",
        "            zeros = zeros.cuda()\n",
        "        inputs = torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous()\n",
        "        targets = samples.data.contiguous().view((-1,))\n",
        "\n",
        "        # calculate the reward\n",
        "        rewards = torch.tensor(rollout.get_reward(samples, args.n_rollout, dis))\n",
        "        if args.cuda:\n",
        "            rewards = rewards.cuda()\n",
        "\n",
        "        # update generator\n",
        "        output = gen(inputs)\n",
        "        loss = pg_loss(output, targets, rewards)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def eval_generator(model, data_iter, criterion, args):\n",
        "    \"\"\"\n",
        "    Evaluate generator with NLL\n",
        "    \"\"\"\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            pred = model(data)\n",
        "            loss = criterion(pred, target)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def train_discriminator(dis, gen, criterion, optimizer, epochs, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args):\n",
        "    \"\"\"\n",
        "    Train discriminator\n",
        "    \"\"\"\n",
        "    generate_samples(gen, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    data_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total_loss = 0.\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = dis(data)\n",
        "            pred = output.data.max(1)[1]\n",
        "            correct += pred.eq(target.data).cpu().sum()\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        data_iter.reset()\n",
        "        avg_loss = total_loss / len(data_iter)\n",
        "        acc = correct.item() / data_iter.data_num\n",
        "        print(\"Epoch {}, train loss: {:.5f}, train acc: {:.3f}\".format(epoch, avg_loss, acc))\n",
        "        dis_adversarial_train_loss.append(avg_loss)\n",
        "        dis_adversarial_train_acc.append(acc)\n",
        "\n",
        "\n",
        "def eval_discriminator(model, data_iter, criterion, args):\n",
        "    \"\"\"\n",
        "    Evaluate discriminator, dropout is enabled\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1)[1]\n",
        "            correct += pred.eq(target.data).cpu().sum()\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    acc = correct.item() / data_iter.data_num\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def adversarial_train(gen, dis, rollout, pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args):\n",
        "    \"\"\"\n",
        "    Adversarially train generator and discriminator\n",
        "    \"\"\"\n",
        "    # train generator for g_steps\n",
        "    print(\"#Train generator\")\n",
        "    for i in range(args.g_steps):\n",
        "        print(\"##G-Step {}\".format(i))\n",
        "        train_generator_PG(gen, dis, rollout, pg_loss, gen_optimizer, args.gk_epochs, args)\n",
        "\n",
        "    # train discriminator for d_steps\n",
        "    print(\"#Train discriminator\")\n",
        "    for i in range(args.d_steps):\n",
        "        print(\"##D-Step {}\".format(i))\n",
        "        train_discriminator(dis, gen, nll_loss, dis_optimizer, args.dk_epochs, \n",
        "            dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "\n",
        "    # update roll-out model\n",
        "    rollout.update_params()\n",
        "\n",
        "def translating_sample(sample_path , voca_path):\n",
        "\n",
        "  print(\"================================================\")\n",
        "  print(\"생성된 데이터 확인.....\")\n",
        "  df = pd.read_csv(sample_path , names = [\"data\"] , sep = \"\\t\" )\n",
        "  Sentences = [sentence.split(\" \") for sentence in df['data'].values]\n",
        "  print(\"생성된 가사 수 : {} , 가사 시퀀스 수 : {} \".format(len(Sentences) , len(Sentences[0])))\n",
        "  print(\" \")\n",
        "\n",
        "  print(\"저장된 단어사전 확인....\")\n",
        "  a = open(voca_path, 'rb')\n",
        "  voca = pickle.load(a)\n",
        "  print(\"단어 사전에 등록된 단어 수 : \" , len(voca))\n",
        "  print(\" \")\n",
        "\n",
        "  # 확인할 무작위 시드 / 사이즈\n",
        "  random_seed = 20\n",
        "  random_size = 10\n",
        "\n",
        "  test_Sentences = Sentences[random_seed:random_seed + random_size]\n",
        "\n",
        "\n",
        "  for sentence in test_Sentences:\n",
        "    real_sentence = \"\"\n",
        "    for s in sentence:\n",
        "\n",
        "      try:\n",
        "        s = int(s)\n",
        "      except: pass\n",
        "\n",
        "      if voca.get(s):\n",
        "        if voca[s] == \"UNK\":\n",
        "          break\n",
        "        real_sentence += voca[s] + \" \"\n",
        "\n",
        "    print(real_sentence)\n",
        "    print(\"------------------------\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4pBdHhdYip6"
      },
      "source": [
        "## 10. SeqGAN 실행\n",
        "\n",
        "- 시간이 많이 소요되므로 가장 아래의 Model Load로 결과 확인하시면 됩니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUxu4VkYpst"
      },
      "source": [
        "### 1) 하이퍼 파라미터 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT4EqrY_rQtu",
        "outputId": "fba6d4a4-30cb-4fb0-83c3-ef7d87a5a590"
      },
      "source": [
        "# Arguemnts\n",
        "parser = argparse.ArgumentParser(description='SeqGAN')\n",
        "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
        "parser.add_argument('--hpc', action='store_true', default=False,\n",
        "                    help='set to hpc mode')\n",
        "parser.add_argument('--data_path', type=str, default='/content/data/', metavar='PATH',\n",
        "                    help='data path to save files (default: /content/data/)')\n",
        "parser.add_argument('--rounds', type=int, default=150, metavar='N',\n",
        "                    help='rounds of adversarial training (default: 150)')\n",
        "parser.add_argument('--g_pretrain_steps', type=int, default=120, metavar='N',\n",
        "                    help='steps of pre-training of generators (default: 120)')\n",
        "parser.add_argument('--d_pretrain_steps', type=int, default=50, metavar='N',\n",
        "                    help='steps of pre-training of discriminators (default: 50)')\n",
        "parser.add_argument('--g_steps', type=int, default=1, metavar='N',\n",
        "                    help='steps of generator updates in one round of adverarial training (default: 1)')\n",
        "parser.add_argument('--d_steps', type=int, default=3, metavar='N',\n",
        "                    help='steps of discriminator updates in one round of adverarial training (default: 3)')\n",
        "parser.add_argument('--gk_epochs', type=int, default=1, metavar='N',\n",
        "                    help='epochs of generator updates in one step of generate update (default: 1)')\n",
        "parser.add_argument('--dk_epochs', type=int, default=3, metavar='N',\n",
        "                    help='epochs of discriminator updates in one step of discriminator update (default: 3)')\n",
        "parser.add_argument('--update_rate', type=float, default=0.8, metavar='UR',\n",
        "                    help='update rate of roll-out model (default: 0.8)')\n",
        "parser.add_argument('--n_rollout', type=int, default=16, metavar='N',\n",
        "                    help='number of roll-out (default: 16)')\n",
        "parser.add_argument('--vocab_size', type=int, default=10365, metavar='N',\n",
        "                    help='vocabulary size (default: 11292)')\n",
        "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
        "                    help='batch size (default: 64)')\n",
        "parser.add_argument('--n_samples', type=int, default=20000, metavar='N',\n",
        "                    help='number of samples gerenated per time (default: 6400)')\n",
        "parser.add_argument('--gen_lr', type=float, default=1e-3, metavar='LR',\n",
        "                    help='learning rate of generator optimizer (default: 1e-3)')\n",
        "parser.add_argument('--dis_lr', type=float, default=1e-3, metavar='LR',\n",
        "                    help='learning rate of discriminator optimizer (default: 1e-3)')\n",
        "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                    help='random seed (default: 1)')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='random seed (default: 1)', metavar='S')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP_yCBl9YlkX"
      },
      "source": [
        "# Parse arguments\n",
        "args = parser.parse_args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "if not args.hpc:\n",
        "    args.data_path = ''\n",
        "\n",
        "# Positive_File / Negative_File\n",
        "POSITIVE_FILE = args.data_path + POSITIVE_FILE\n",
        "NEGATIVE_FILE = args.data_path + NEGATIVE_FILE"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfr7klavYv2X"
      },
      "source": [
        "### 2) 모델 로스 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnvjujWZYu5H"
      },
      "source": [
        "# Set models, criteria, optimizers\n",
        "generator = Generator(args.vocab_size, g_embed_dim, g_hidden_dim, args.cuda)\n",
        "discriminator = Discriminator(d_num_class, args.vocab_size, d_embed_dim, d_filter_sizes, d_num_filters, d_dropout_prob)\n",
        "target_lstm = TargetLSTM(args.vocab_size, g_embed_dim, g_hidden_dim, args.cuda)\n",
        "\n",
        "nll_loss = nn.NLLLoss()\n",
        "pg_loss = PGLoss()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trOw8CSjY5b6"
      },
      "source": [
        "### 3) cuda/optimizer 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm71wnz9Y5EZ"
      },
      "source": [
        "if args.cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    target_lstm = target_lstm.cuda()\n",
        "    nll_loss = nll_loss.cuda()\n",
        "    pg_loss = pg_loss.cuda()\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "gen_optimizer = optim.Adam(params=generator.parameters(), lr=args.gen_lr)\n",
        "dis_optimizer = optim.SGD(params=discriminator.parameters(), lr=args.dis_lr)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1djRuZW5ZB9r"
      },
      "source": [
        "### 4) Pretrain Generator / Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIY3AtPuZA3e"
      },
      "source": [
        "# Container of experiment data\n",
        "gen_pretrain_train_loss = []\n",
        "gen_pretrain_eval_loss = []\n",
        "dis_pretrain_train_loss = []\n",
        "dis_pretrain_train_acc = []\n",
        "dis_pretrain_eval_loss = []\n",
        "dis_pretrain_eval_acc = []\n",
        "gen_adversarial_eval_loss = []\n",
        "dis_adversarial_train_loss = []\n",
        "dis_adversarial_train_acc = []\n",
        "dis_adversarial_eval_loss = []\n",
        "dis_adversarial_eval_acc = []"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynz5thM-SEw6",
        "outputId": "7231f4f8-2a1a-4038-8738-7850590d87f7"
      },
      "source": [
        "# # Generate toy data using target LSTM\n",
        "# print('#####################################################')\n",
        "# print('Generating data ...')\n",
        "# print('#####################################################\\n\\n')\n",
        "# generate_samples(generator, args.batch_size, args.n_samples, POSITIVE_FILE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "Generating data ...\n",
            "#####################################################\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djId_5tiLemC"
      },
      "source": [
        "### 5) Pretrain Generator with real data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9oNZtBfCR5C"
      },
      "source": [
        "- 시간이 꽤 소요되므로 아래에서 모델을 로드하시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f7BYA8hZI_f"
      },
      "source": [
        "MODEL_PATH = \"../Param/4_SeqGAN.pt\"\n",
        "VOCA_PATH = \"../Data/4_SeqGAN_Data/4_SeqGAN_idx2pos.pkl\"\n",
        "GENERATE_PATH = \"../Data/4_SeqGAN_Data/4_SeqGAN_negative.txt\"\n",
        "\n",
        "# Pre-train generator using MLE\n",
        "print('#####################################################')\n",
        "print('Start pre-training generator with MLE...')\n",
        "print('#####################################################\\n')\n",
        "gen_data_iter = GenDataIter(POSITIVE_FILE, args.batch_size)\n",
        "\n",
        "for i in range(args.g_pretrain_steps):\n",
        "# for i in range(50):      \n",
        "    print(\"G-Step {}\".format(i))\n",
        "    train_generator_MLE(generator, gen_data_iter, nll_loss, \n",
        "        gen_optimizer, args.gk_epochs, \n",
        "        gen_pretrain_train_loss, args)\n",
        "    generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "    gen_loss = eval_generator(generator, eval_iter, nll_loss, args)\n",
        "    gen_pretrain_eval_loss.append(gen_loss)\n",
        "    print(\"eval loss: {:.5f}\\n\".format(gen_loss))\n",
        "    translating_sample(GENERATE_PATH , VOCA_PATH)\n",
        "\n",
        "print('#####################################################\\n\\n')\n",
        "\n",
        "print(\"Pretrain Generator Model Save...!\")\n",
        "torch.save(generator , MODEL_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHQxeY2AAwnv"
      },
      "source": [
        "### 6) Model Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V5f3e94omNI"
      },
      "source": [
        "MODEL_PATH = \"../Param/4_SeqGAN.pt\"\n",
        "\n",
        "generator = torch.load(MODEL_PATH)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5vSjLjWCO2T"
      },
      "source": [
        "- 모델 성능 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5zmruKqo0bL"
      },
      "source": [
        "GENERATE_PATH = \"../Data/4_SeqGAN_Data/4_SeqGAN_negative.txt\"\n",
        "\n",
        "generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "gen_loss = eval_generator(generator, eval_iter, nll_loss, args)\n",
        "gen_pretrain_eval_loss.append(gen_loss)\n",
        "print(\"eval loss: {:.5f}\\n\".format(gen_loss))\n",
        "\n",
        "translating_sample(GENERATE_PATH , VOCA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxKe9JGQEjhG"
      },
      "source": [
        "### 7) Pretrain Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRyuHVLrUmJX"
      },
      "source": [
        "# Pre-train discriminator\n",
        "print('#####################################################')\n",
        "print('Start pre-training discriminator...')\n",
        "print('#####################################################\\n')\n",
        "for i in range(20):\n",
        "#for i in range(args.d_pretrain_steps):      \n",
        "    print(\"D-Step {}\".format(i))\n",
        "    train_discriminator(discriminator, generator, nll_loss, \n",
        "        dis_optimizer, args.dk_epochs, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "    generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "    dis_loss, dis_acc = eval_discriminator(discriminator, eval_iter, nll_loss, args)\n",
        "    dis_pretrain_eval_loss.append(dis_loss)\n",
        "    dis_pretrain_eval_acc.append(dis_acc)\n",
        "    print(\"eval loss: {:.5f}, eval acc: {:.3f}\\n\".format(dis_loss, dis_acc))\n",
        "print('#####################################################\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXwecpmgaBbV"
      },
      "source": [
        "### 8) Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "578CQJE4ZlIG"
      },
      "source": [
        "# Adversarial training\n",
        "print('#####################################################')\n",
        "print('Start adversarial training...')\n",
        "print('#####################################################\\n')\n",
        "rollout = Rollout(generator, args.update_rate)\n",
        "# for i in range(10):    \n",
        "for i in range(args.rounds):\n",
        "    print(\"Round {}\".format(i))\n",
        "    adversarial_train(generator, discriminator, rollout, \n",
        "        pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "    generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    gen_eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "    dis_eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "    gen_loss = eval_generator(generator, gen_eval_iter, nll_loss, args)\n",
        "    gen_adversarial_eval_loss.append(gen_loss)\n",
        "    dis_loss, dis_acc = eval_discriminator(discriminator, dis_eval_iter, nll_loss, args)\n",
        "    dis_adversarial_eval_loss.append(dis_loss)\n",
        "    dis_adversarial_eval_acc.append(dis_acc)\n",
        "    translating_sample(GENERATE_PATH , VOCA_PATH)\n",
        "    print(\"gen eval loss: {:.5f}, dis eval loss: {:.5f}, dis eval acc: {:.3f}\\n\"\n",
        "        .format(gen_loss, dis_loss, dis_acc))\n",
        "\n",
        "# Save experiment data\n",
        "with open(args.data_path + 'experiment.pkl', 'wb') as f:\n",
        "    pickle.dump(\n",
        "        (gen_pretrain_train_loss,\n",
        "            gen_pretrain_eval_loss,\n",
        "            dis_pretrain_train_loss,\n",
        "            dis_pretrain_train_acc,\n",
        "            dis_pretrain_eval_loss,\n",
        "            dis_pretrain_eval_acc,\n",
        "            gen_adversarial_eval_loss,\n",
        "            dis_adversarial_train_loss,\n",
        "            dis_adversarial_train_acc,\n",
        "            dis_adversarial_eval_loss,\n",
        "            dis_adversarial_eval_acc),\n",
        "        f,\n",
        "        protocol=pickle.HIGHEST_PROTOCOL\n",
        "    )\n",
        "\n",
        "# save GAN generator\n",
        "torch.save(generator, \"../Param/4_SeqGAN.pt\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq_TPQ-SGjgM"
      },
      "source": [
        "### 9) Model Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1QbpH2QzBas"
      },
      "source": [
        "generator = torch.load(\"../Param/4_SeqGAN.pt\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-3LHU6RxMIz",
        "outputId": "199b2bdb-6c47-44c2-c39c-3186e90f4da1"
      },
      "source": [
        "GENERATE_PATH = \"../Data/4_SeqGAN_Data/4_SeqGAN_negative.txt\"\n",
        "\n",
        "generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "gen_loss = eval_generator(generator, eval_iter, nll_loss, args)\n",
        "gen_pretrain_eval_loss.append(gen_loss)\n",
        "print(\"eval loss: {:.5f}\\n\".format(gen_loss))\n",
        "\n",
        "translating_sample(GENERATE_PATH , VOCA_PATH)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eval loss: 2.23101\n",
            "\n",
            "================================================\n",
            "생성된 데이터 확인.....\n",
            "생성된 가사 수 : 19968 , 가사 시퀀스 수 : 20 \n",
            " \n",
            "저장된 단어사전 확인....\n",
            "단어 사전에 등록된 단어 수 :  10365\n",
            " \n",
            "사랑 해요 내 마음 을 흩어 아쉬워 봐 허나 아무 일도 도 체 의 가슴속 에서 다른 바람 에 \n",
            "------------------------\n",
            "널 상 마저도 해야 할 텐데 내 가 돈 하지 할 텐데 \n",
            "------------------------\n",
            "이 곳 많은 향기 만은 보게 있어 모두 더욱 소리 는 불 을 \n",
            "------------------------\n",
            "다 돌아올지 안식 을 주는 운명 짓나요 \n",
            "------------------------\n",
            "겨울 이유 라도 \n",
            "------------------------\n",
            "우리 의 바람 저 끝 에 우우 \n",
            "------------------------\n",
            "사랑 은 싶다니까 나도 내게 내게 걷다 막연한 길이 잖아요 내게 게 뿐일세 키 있었는지도 안될 것 도 마 \n",
            "------------------------\n",
            "담장 에 분홍 바람 하늘 다시 마냥 행복한 곳 끝 까지 \n",
            "------------------------\n",
            "다시 돌아온 그 별거 하고 애써 웃게 몰아치는 그 많았지만 을 당신 이 아니길 빌어 \n",
            "------------------------\n",
            "미뤄 그을린 내 마음 을 봐 난 잠시 결말 을 뻗는 단 한 모습 에 \n",
            "------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N83BAw8lMd6i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}